{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import scipy.signal as scisig\n",
    "\n",
    "np.set_printoptions(linewidth=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "We will be implementing and doing a cost analysis of [Winograd Convolution](https://arxiv.org/pdf/1509.09308.pdf).\n",
    "\n",
    "We will be implementing a $F(m,r)$ filter. \n",
    "\n",
    "### 1-D Filter F(2,3)\n",
    "\n",
    "Given a $r=3$ sized filter, we'd like to have an output of size $m=2$. This can be done with the algorithm\n",
    "\n",
    "$$Y = A^T \\Big[(Gg) \\odot (B^Td) \\Big]$$\n",
    "\n",
    "We define $A^T, G, B$ below. Here, $d$ is the data and $g$ is the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "B23 = np.asarray([\n",
    "    [1, 0,-1, 0],\n",
    "    [0, 1, 1, 0],\n",
    "    [0,-1, 1, 0],\n",
    "    [0, 1, 0,-1]\n",
    "]).T\n",
    "\n",
    "G23 = np.asarray([\n",
    "    [ 1,  0, 0],\n",
    "    [.5, .5,.5],\n",
    "    [.5,-.5,.5],\n",
    "    [ 0, 0,  1]\n",
    "])\n",
    "\n",
    "A23 = np.asarray([\n",
    "    [1,1,1,0],\n",
    "    [0,1,-1,-1]\n",
    "]).T\n",
    "\n",
    "### F(4x4,3x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For full convolution, we need a stride length $s=r-1=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1.3951435055142857e-16\n"
     ]
    }
   ],
   "source": [
    "n = 2**10\n",
    "\n",
    "g = np.random.random(3)\n",
    "d = np.random.random(n)\n",
    "y = np.zeros(0)\n",
    "\n",
    "d2 = np.append(np.zeros(2),d)\n",
    "d2 = np.append(d2, np.zeros(2))\n",
    "for i in range(len(d2)//2-1):\n",
    "    yTemp = np.dot(A23.T, (G23 @ g) * (B23.T @ d2[i*2:(i*2)+4]) )\n",
    "    y = np.append(y,yTemp)\n",
    "\n",
    "convLib = np.convolve(d,g[::-1])\n",
    "\n",
    "print(\"Error:\",la.norm(y-convLib)/la.norm(convLib))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Analysis\n",
    "\n",
    "$A^T$, the Winograd domain inversion, is a $2 \\times 4$ matrix. $G$, the filter transformation is a $4 \\times 3$. $B$, the data transformation, is a $4 \\times 4$ matrix.\n",
    "\n",
    "So in each of the small filters, we will require \n",
    "1. $(4 \\times 3, 4 \\times 3)$ flops for filter transformation (Only need to be done ONCE)\n",
    "1. $(4 \\times 4, 4 \\times 4)$ flops for data transformation\n",
    "1. $(0,4)$ flops for pointwise multiplication. This is the big $\\alpha = m + r -1$ multiplies the Winograd paper points out\n",
    "1. $(2 \\times 4, 2 \\times 4)$ flops for transformation back\n",
    "\n",
    "So in total, we will need $(24,28)$ flops for each $F(2,3)$ filter. Let $\\alpha = (m+r-1)$. We can generalize the cost in flops as:\n",
    "\n",
    "$$( \\ \\alpha ( \\alpha + m) \\ , \\ \\alpha(\\alpha + m + 1 ) \\ )$$\n",
    "\n",
    "We purposefully leave out the setup filter which cost $(\\alpha r, \\ \\alpha r)$. For our specific $F(2,3)$, this turns out to be\n",
    "\n",
    "$$(24,28) \\textrm{ flops}$$\n",
    "\n",
    "Quick analysis shows this is definitely worse than a standard $(4,6)$ flops for the direct method. However, we will see later in the 2-D case the savings are even better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-D Convolution F(2x2, 3x3)\n",
    "\n",
    "2-D convolution is expanded to\n",
    "\n",
    "$$Y = A^T \\Big[(GgG^T) \\odot (B^TdB) \\Big]A$$\n",
    "\n",
    "We implement Winograd's Minimal Filtering Algorithm for 2D convolution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume N=1 image, K=1 filter, C=1 channel\n",
    "def simpleWinogradAlg(g,d,m,B,G,A):\n",
    "    N = K = C = 1\n",
    "    \"\"\"\n",
    "    @g: 2d.numpy_array as square filter\n",
    "    @d: 2d.numpy_array as data\n",
    "    @m: int as output of FIR filter F(m,r)\n",
    "    \"\"\"\n",
    "    h,w = d.shape\n",
    "    r = g.shape[0]\n",
    "    \n",
    "    assert(g.shape[0] == g.shape[1])\n",
    "    assert(h%m == 0 and w%m == 0)\n",
    "    \n",
    "    h-=m; w-=m\n",
    "    \n",
    "    P = (h//m)*(w//m) # num of tiles\n",
    "    a = m+r-1 # input tile size\n",
    "    \n",
    "    dChunks = np.zeros((C,P,a,a))\n",
    "    for c in range(C):\n",
    "        for y in range(h//m):\n",
    "            for x in range(w//m):\n",
    "                b = y*(w//m) + x\n",
    "                dChunks[c,b] = d[(y*m):(y*m)+a, (x*m):(x*m)+a]\n",
    "    \n",
    "    print(a,K,C)\n",
    "    U = np.zeros((a,a,K,C))\n",
    "    for k in range(K):\n",
    "        for c in range(C):\n",
    "            uInterm = np.dot(G, np.dot(g, G.T))\n",
    "            for e in range(a):\n",
    "                for v in range(a):\n",
    "                    U[e,v,k,c] = uInterm[e,v]\n",
    "            \n",
    "    print(a,C,P)\n",
    "    V = np.zeros((a,a,C,P))\n",
    "    for b in range(P):\n",
    "        for c in range(C):\n",
    "            vInterm = np.dot(B.T, np.dot(dChunks[c,b], B))\n",
    "            for e in range(a):\n",
    "                for v in range(a):\n",
    "                    V[e,v,c,b] = vInterm[e,v]\n",
    "            \n",
    "    M = np.zeros((a,a,K,P))\n",
    "    for e in range(a):\n",
    "        for v in range(a):\n",
    "            M[e,v] = np.dot(U[e,v], V[e,v])\n",
    "            \n",
    "    Y = np.zeros((K,P,m,m))\n",
    "    for k in range(K):\n",
    "        for b in range(P):\n",
    "            mInterm = np.zeros((a,a))\n",
    "            for e in range(a):\n",
    "                for v in range(a):\n",
    "                    mInterm[e,v] = M[e,v,k,b]         \n",
    "            Y[k,b] = np.dot(A.T, np.dot(mInterm, A))\n",
    "        \n",
    "    Ynew = np.zeros((K,h,w))\n",
    "    for k in range(K):\n",
    "        for y in range(h//m):\n",
    "            for x in range(w//m):\n",
    "                b = y*(w//m) + x\n",
    "                Ynew[k,y*m:(y+1)*m, x*m:(x+1)*m] = Y[k,b]\n",
    "    return Ynew\n",
    "\n",
    "def padImage(g,r):\n",
    "    h,w = g.shape\n",
    "    g2 = np.zeros((2*r-2 + h,2*r-2 + w))\n",
    "    g2[r-1:r-1+h,r-1:r-1+w] = g\n",
    "    return g2\n",
    "\n",
    "def revMatrix(M):\n",
    "    n1,n2 = M.shape\n",
    "    return np.eye(n1)[::-1] @ M @ np.eye(n2)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 356 ms, sys: 2.05 ms, total: 358 ms\n",
      "Wall time: 359 ms\n",
      "4 1 1\n",
      "4 1 4225\n",
      "CPU times: user 87.5 ms, sys: 927 Âµs, total: 88.4 ms\n",
      "Wall time: 88.8 ms\n",
      "Error: 1.9056215930084894e-16\n"
     ]
    }
   ],
   "source": [
    "n = 2**7\n",
    "r = 3\n",
    "f = np.random.random((r,r))\n",
    "g = np.random.random((n,n))\n",
    "%time c = scisig.convolve2d(f,g)\n",
    "\n",
    "g2 = revMatrix(g)\n",
    "g2 = padImage(g2,r)\n",
    "%time cWino = simpleWinogradAlg(f,g2,2,B23,G23,A23)[0]\n",
    "cWino = revMatrix(cWino)\n",
    "\n",
    "print(\"Error:\",la.norm(c - cWino)/la.norm(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Analysis\n",
    "\n",
    "Assume a filter size $R \\times r$ and input of size $H \\times W$. Let $N$ be the number of images, $K$ be the number of filters, and $C$ be the number of channels. Define $\\alpha = R+m-1$.\n",
    "\n",
    "#### Direct\n",
    "\n",
    "The direct method would just do a series of matrix element-wise multiplies and sum them up.\n",
    "\n",
    "In total, we expect there to be approximately $HW$ (slightly larger due to padding, but we will say this is negligible) of these multiplies with the $R \\times R$ filter. For each convolution, we need to add up the neighboring multiplies. We expect again approximately $HW$ of these. In each, we should expect $(R-1)^2$ additions required.\n",
    "\n",
    "For one filter, one image, and one channel, we bound the number of additions and multiplies, denoted as $( \\cdot, \\cdot)$ to\n",
    "\n",
    "$$(0,HWR^2) + (HWR^2,HWR^2)$$\n",
    "\n",
    "Over the entire algorithm, the direct algorithm will incur a cost of\n",
    "\n",
    "$$(CNKHWR^2,2 \\cdot CNKHWR^2) $$\n",
    "\n",
    "#### Winograd\n",
    "\n",
    "The algorithm involves four steps:\n",
    "1. Filter Transformation\n",
    "1. Data Transformation\n",
    "1. Multiplication in Winograd Space\n",
    "1. Winograd Space Inversion\n",
    "\n",
    "For the **filter transformation**, this is only done once per filter per channel. For each filter+channel combination, this incurs a $(\\alpha \\times R \\times R) + (R \\times \\alpha \\times \\alpha)$ matrix multiplication.\n",
    "\n",
    "For the **data transformation**, this is done once per image per filter per channel. For each slice of the image+channel combination, we perform two $\\alpha \\times \\alpha \\times \\alpha$ matrix multiplications\n",
    "\n",
    "For the **multiplication**, this is done once per image per channel. This is done $\\alpha^2$ times and involves a $K \\times C \\times P$ matrix multiplication.\n",
    "\n",
    "For the **inversion**, this is done once per image per filter per channel. For each image slice+filter combination, we perform a $(m \\times \\alpha \\times \\alpha) + (\\alpha \\times \\alpha \\times m)$ matrix multiplication.\n",
    "\n",
    "In total, we see the cost of just addition and multiplications is\n",
    "\n",
    "$$ KC \\Big( M(\\alpha,R,R) + M(R,\\alpha,\\alpha) \\Big) + 2\\frac{NHW}{m^2}M(\\alpha, \\alpha, \\alpha) + \\alpha^2NM(K,C,\\frac{HW}{m^2}) + \\frac{KNHW}{m^2} \\Big( M(m, \\alpha, \\alpha) + M(\\alpha, \\alpha, m ) \\Big) $$\n",
    "\n",
    "where $M(\\cdot,\\cdot,\\cdot)$ represents the cost of a matrix-matrix multiplication with those axis sizes. Here we let $P = \\frac{HW}{m^2}$.\n",
    "\n",
    "Since $m \\ge 1$ and $r \\ge 1$, we trivially show $\\alpha \\ge r$ and $\\alpha \\ge m$, or that $\\alpha$ dominates $r$ and $m$. A cleaner variation is\n",
    "\n",
    "$$\\le 2KC \\cdot M(R,\\alpha,\\alpha) + 2\\frac{NHW}{m^2} \\cdot M(\\alpha, \\alpha, \\alpha) + \\alpha^2N \\cdot M(K,C,\\frac{HW}{m^2}) + 2\\frac{KNHW}{m^2} \\cdot M(\\alpha, \\alpha, m ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleWinogradAlg_FLOPS(h,w,r,m,B,G,A,N,K,C,matmul):\n",
    "    assert(h%m == 0 and w%m == 0)\n",
    "    \n",
    "    h-=2; w-=2\n",
    "    \n",
    "    P = (h//m)*(w//m) # num of tiles\n",
    "    a = m+r-1 # input tile size\n",
    "    \n",
    "    dChunks = np.zeros((C,P,a,a))\n",
    "    \n",
    "    flops = np.zeros(2)\n",
    "    \n",
    "    U = np.zeros((a,a,K,C))\n",
    "    g = np.zeros((r,r))\n",
    "    temp = K * C * ( matmul(G,g) + matmul(g,G.T))\n",
    "    flops += temp\n",
    "            \n",
    "    V = np.zeros((a,a,C,P))\n",
    "    temp = N * P * C * ( matmul(B.T, dChunks[0,0]) + matmul(dChunks[0,0],B) )\n",
    "    flops += temp\n",
    "            \n",
    "    M = np.zeros((a,a,K,P))\n",
    "    # (K,C) x (C,P)\n",
    "    temp = N * a * a * matmul(U[0,0],V[0,0])\n",
    "    flops += temp\n",
    "            \n",
    "    Y = np.zeros((K,P,m,m))\n",
    "    mInterm = np.zeros((a,a))\n",
    "    temp = K * P * ( matmul(A.T, mInterm) + matmul(mInterm, A) )\n",
    "    flops += temp\n",
    "        \n",
    "    # reorder\n",
    "    return flops\n",
    "\n",
    "def directMatmul(M1,M2):\n",
    "    assert(M1.shape[1] == M2.shape[0])\n",
    "    return M1.shape[0] * M1.shape[1] * M2.shape[1]\n",
    "\n",
    "def direct2DConvFlops(C,N,K,H,W,R):\n",
    "    return np.asarray([C*N*K*H*W*(R**2),2*C*N*K*H*W*(R**2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct Flops: [2283798528 4567597056]\n",
      "Winograd Flops: [1.7288329e+09 1.7288329e+09]\n",
      "Winograd Savings: [1.32100594 2.64201188]\n"
     ]
    }
   ],
   "source": [
    "N = 1\n",
    "K = 96 # higher means more savings for multiplications\n",
    "C = 3\n",
    "\n",
    "R = 11\n",
    "M = 2\n",
    "p = 8\n",
    "H = M**p\n",
    "W = M**p\n",
    "\n",
    "a = M + R - 1\n",
    "B = np.zeros((a,a)).T\n",
    "G = np.zeros((a,R))\n",
    "A = np.zeros((M,a)).T\n",
    "# h,w,r,m,B,G,A,N,K,C,matmul\n",
    "winoFlops = simpleWinogradAlg_FLOPS(H,W,R,M,B,G,A,N,K,C,directMatmul)\n",
    "# C,N,K,H,W,R\n",
    "directFlops = direct2DConvFlops(C,N,K,H,W,R)\n",
    "\n",
    "lowest = H*W*K*N*C\n",
    "\n",
    "print(\"Direct Flops:\",directFlops)\n",
    "print(\"Winograd Flops:\",winoFlops)\n",
    "\n",
    "print(\"Winograd Savings:\",directFlops/winoFlops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the Costs\n",
    "\n",
    "We will use the filter $F(2 \\times 2, 3 \\times 3)$ and $\\alpha = 3+2-1=4$.\n",
    "\n",
    "In the direct method, we have a cost of about \n",
    "\n",
    "$$(9KCNHW,18 \\cdot KCNHW) $$\n",
    "\n",
    "For Winograd, we have\n",
    "\n",
    "$$2KC \\cdot M(3,4,4) + \\frac{NHW}{2} \\cdot M(4,4,4) + 16N \\cdot M(K,C,\\frac{HW}{4}) + \\frac{KNHW}{2} \\cdot M(4,4,2) $$\n",
    "\n",
    "Using just direct matrix-multiplication, the cost here is then\n",
    "\n",
    "$$\\le 2KC \\cdot 48 + 32NHW + 4KCNHW + 16KNHW$$\n",
    "\n",
    "Here, let $C=3$ for the typical RGB filter, where the cost is reduced further to:\n",
    "\n",
    "$$(27KNHW,54KNHW) $$\n",
    "\n",
    "and \n",
    "\n",
    "$$288K + 32NHW + 12KNHW + 16KNHW$$\n",
    "\n",
    "Given a sufficiently high filter count $K > 1$, we can expect a speed up of approximately $\\frac{54}{28} \\approx 2$ with the number of multiplications.\n",
    "\n",
    "### Winograd's Saving\n",
    "\n",
    "In the paper, we see this algorithm requires only $16$ multiplications compared to $36$. This is a saving of about $36/16 = 2.25$, which is approximately what we see in practice.\n",
    "\n",
    "Typically, for a $F(m \\times m, r \\times r)$ filter, we should see a saving of $\\frac{m^2r^2}{(m+r-1)^2}$. The larger $m,r$ can go, the better the savings we should expect. However, this can occur more constant additions and instability. For future work, we can investigate how to improve this using our current work with Toom-Cook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
